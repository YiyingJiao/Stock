{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Yiying Jiao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this function takes in a path which leads to a csv file in my local directory\n",
    "#strip the dataframe to only a date column and an url column, of unique rows that only mention\n",
    "#apple(as a whole word)\n",
    "\n",
    "def proccess_csv (path):\n",
    "    df = pd.read_csv(path, on_bad_lines=\"skip\", sep='\\t')\n",
    "\n",
    "\n",
    "    #--------extract relevanet columns------\n",
    "    df = df.iloc[:, [1, (len(df.columns)-1)]]# Selects the 2nd and last columns\n",
    "\n",
    "    # Step 1: Save the current column row (the mistaken column names)\n",
    "    header_values = df.columns.tolist()  #get the current columns names(who are the first row)\n",
    "\n",
    "    # Step 2: Create a DataFrame row from the column values\n",
    "    header_df = pd.DataFrame([header_values], columns=df.columns)\n",
    "\n",
    "    # Step 3: Concatenate the first row with the original DataFrame\n",
    "    df_corrected = pd.concat([header_df, df], ignore_index=True)\n",
    "\n",
    "    # Now assign new column names\n",
    "    new_column_names = [\"date\", \"url\"]\n",
    "    df_corrected.columns = new_column_names\n",
    "\n",
    "    #rename for similicity\n",
    "    df=df_corrected.copy()\n",
    "\n",
    "    #----------filter---------\n",
    "    #filter for mention of apple(as a whole word)\n",
    "    filtered_df = df[df['url'].str.contains(r'\\bapple\\b', case=False, na=False, regex=True)]\n",
    "    \n",
    "    #filter to get rid of all duplicate links\n",
    "    filtered_df = filtered_df.copy()\n",
    "    filtered_df.drop_duplicates(subset='url', keep=\"first\", inplace=True)\n",
    "    \n",
    "    #drop the index so it can be ready to concatenate into the big dataframe\n",
    "    filtered_df = filtered_df.reset_index(drop=True)\n",
    "    return filtered_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Directory containing the CSV files\n",
    "directory_path = \"/Users/jiao/projects/csv_files_2023\"\n",
    "\n",
    "# Get all CSV files in the directory in a list\n",
    "csv_file_list = [f for f in os.listdir(directory_path) if f.endswith(\".CSV\")]\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "# iterate through each csv, process each and save output\n",
    "for file in csv_file_list:\n",
    "    file_path = os.path.join(directory_path, file)  # Full path to the file\n",
    "    # run the functionn, process the CSV\n",
    "    curr_df = proccess_csv(file_path)\n",
    "    urls = curr_df['url']\n",
    "    curr_df['url'].fillna('', inplace=True)\n",
    "    main_df = pd.concat([main_df, curr_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv(\"/Users/jiao/projects/23_main_df.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
